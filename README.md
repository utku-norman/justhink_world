# JUSThink World

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


## Overview

This repository contains the [justhink_world] Python package to represent and visualise activities in a pedagogical scenario that contains a human-robot collaborative learning activity for school children, named [JUSThink](https://www.epfl.ch/labs/chili/index-html/research/animatas/justhink/). The scenario aims to improve their computational thinking skills by applying abstract and algorithmic reasoning to solve an unfamiliar problem on networks.

The scenario consists of individual (e.g. as in a test for assessment) and collaborative (with a robot) activities.

* In an individual activity, a human learner is given a network of gold mines with possible positions for railway tracks, where each track if it is built connects one mine to another. The cost of each track is visible. The goal is to collect the gold by connecting the gold mines to each other, while spending as little as possible to build the tracks.
* In a collaborative activity, the human and the robot as (same-status) peers collaboratively construct a solution to this problem by deciding together which tracks to build, and submit it as their solution to the system. They take turns in suggesting to select/pick a specific connection, where the other either agrees or disagrees with this suggestion. A track will be built only if it is suggested by one and accepted by the other.

A human learner participates in the pedagogical scenario through an application ([justhink_scenario]). The robot behaviour is generated by [justhink_agent] and manifested (on e.g. [QTrobot]) by [justhink_robot]. These are [ROS] nodes that communicate via the custom ROS messages and services defined in [justhink_msgs].

A `world` in [justhink_world] is used to represent an activity via describing an interaction between an agent and its environment, following the design principles [here](https://h2r.github.io/pomdp-py/html/design_principles.html) [[1]](#references). Briefly:

* a `state` represents the state of the world: it contains information on the current solution (i.e. the selected connections in the network), the agents that can take actions (for turn-taking), the current attempt number etc.
* an `environment` maintains the state of the world via state transitions defined in a `transition model`.
* an `agent` operates in this environment by taking `action`s, receiving observations (fully observable), and updating its belief.
Furthermore, an agent can list the available actions for a given state via its `policy model`.

Conceretely, the current solution's state for a given activity is an instance of `NetworkState`, as part of the complete specification of the activity state as an `Env(ironment)State` (see [state.py](justhink_world/domain/state.py)). 
State transitions (i.e. <state, action, next state\> triples) for an individual activity is defined in `IndividualTransitionModel`, and for a collaborative activity in `CollaborativeTransitionModel` (see [transition_model.py](justhink_world/models/transition_model.py)).
The action space of the available actions at a given state are determined by `IndividualPolicyModel` and `CollaborativePolicyModel` (see [policy_model.py](justhink_world/models/policy_model.py)).

**Keywords:** artificial intelligence, human-robot interaction, mutual understanding, collaborative learning, computational thinking

### License

The whole package is under MIT License, see [LICENSE](LICENSE).

This README is based on the project [ros_best_practices](https://github.com/leggedrobotics/ros_best_practices), Copyright 2015-2017, PÃ©ter Fankhauser. It is licensed under the BSD 3-Clause Clear License. See [doc/LICENSE](doc/LICENSE) for additional details.

**Author: Utku Norman<br />
Affiliation: [CHILI Lab, EPFL](https://www.epfl.ch/labs/chili/)<br />
Maintainer: Utku Norman, utku.norman@epfl.ch**

The [justhink_world] package has been tested under Python 3.8 on Ubuntu 20.04.
This is research code, expect that it changes often and any fitness for a particular purpose is disclaimed.


<img src="doc/collab_activity.jpg" width="768" />


### Publications

If you use this work in an academic context, please cite the following publication(s):

* U. Norman, B. Bruno, and P. Dillenbourg, **Mutual Modelling Ability for a Humanoid Robot: How can it improve my learning as we solve a problem together?,** in Robots for Learning Workshop in 16th annual IEEE/ACM Conference on Human-Robot Interaction (HRI 2021). ([PDF](http://infoscience.epfl.ch/record/283614))

        @inproceedings{norman_mutual_2021,
            author = {Norman, Utku and Bruno, Barbara and Dillenbourg, Pierre},
            booktitle = {Robots for Learning Workshop in 16th annual {IEEE}/{ACM} Conference on Human-Robot Interaction ({HRI} 2021)},
            title = {Mutual Modelling Ability for a Humanoid Robot: How can it improve my learning as we solve a problem together?},
            url = {http://infoscience.epfl.ch/record/283614},
            year = {2021},
        }


## Installation

### Building from Source

#### Dependencies

* [pomdp_py](https://h2r.github.io/pomdp-py/html/) to describe the world/problem as an agent interacting with its environment [[1]](#references)
* [networkx](https://networkx.org/) to represent and reason with the networks in an activity
* [pyglet](https://pyglet.readthedocs.io/en/latest/) to visualise and interact with the activity from a role (human or the robot)
* [importlib_resources](https://importlib-resources.readthedocs.io/en/latest/) to access to the resources like the images
* [pqdict](https://pypi.org/project/pqdict/) to implement a priority queue, used in the Prim's algorithm


#### Building

1) Clone this ([justhink_world]) repository:
```
git clone https://github.com/utku-norman/justhink_world.git
```

2) Create a new [virtual environment](https://docs.python.org/3/tutorial/venv.html) and activate it (can do so in the same folder. Note that the folder name `venv` is [git-ignored](https://git-scm.com/docs/gitignore)):
```
cd justhink_world
python3 -m venv venv
source venv/bin/activate
```

If you do not have `venv`, first install it by: `sudo apt install python3-venv`

3) Install this package, along with the remaining dependencies via `pip` (in '-e' i.e. editable mode for "developer mode")
```
pip install -e .
```

If you encounter an error regarding Pygraphviz while installing pomdp_py, first install its dependencies (as in [here](https://pygraphviz.github.io/documentation/stable/install.html)): `sudo apt install graphviz graphviz-dev; pip install pygraphviz`

4) Check the installation by running the following in a Python interpreter:
```
from justhink_world import list_worlds, create_all_worlds
worlds = create_all_worlds()
```

## Usage

### Print the list of available worlds and initialise all these worlds.
```
from justhink_world import list_worlds, create_all_worlds

print(list_worlds())

worlds = create_all_worlds()

for name, world in worlds.items():
    print(name, world)
```

### Display an activity state in a window (non-interactive).
```
from justhink_world import create_world, show_state
from justhink_world.domain.action import PickAction

# Create a world.
world = create_world('pretest-1')

# Take actions.
world.act(PickAction((3, 1)))
world.act(PickAction((1, 4)))

# Display the state of the world.
show_state(world.cur_state)
```

You will see a window with two selected edges as follows:


<img src="doc/example_show_state.jpg" width="768" />


### Try out a world (interactive).

Shortcuts to navigate through and modify the activity:
* Press `ESCAPE` key to close the application,
* Use `LEFT`-`RIGHT` keys to navigate to the previous and the next state respectively,
* Use `HOME`-`END` keys to navigate to the first and the last state of the history respectively,
* Use `P` to toggle pause (i.e. no action is allowed), and `TAB` to toggle the role (between the human and the robot).
* Note that taking an action at a state permanently removes the earlier future history (i.e. all the states that are later than that state), while keeping the previous history (all the states up to that state).

#### Try out an individual (i.e. a test) world.

How to interact with an individual world:
* You can select a connection by clicking on a gold mine, dragging to another gold mine and releasing on that gold mine.
* You can delete all selected connections by pressing the erase button.

##### Visualise an individual world.
```
from justhink_world import create_world, show_world

# Create a world.
world = create_world('pretest-1')

# Visualise the world on the last attached screen (by default).
show_world(world)



# Visualise the world with drawing mode 'click'.
# show_world(world, drawing_mode='click')

# Visualise the world on the current screen.
# show_world(world, screen_index=0)
```



From the viewpoint of the robot.
```
from justhink_world import create_world, show_world

world = create_world('robot-individual-1')

show_world(world, drawing_mode='click')
```


##### Execute actions via Python commands (and visualise the world at its latest state).
```
from justhink_world import create_world, show_world
from justhink_world.domain.action import PickAction, ClearAction, \
AttemptSubmitAction, ContinueAction, SubmitAction

# Create a world.
world = create_world('pretest-1')

# Act on the world.
world.act(PickAction((3, 1)))
world.act(PickAction((1, 4)))
world.act(AttemptSubmitAction())
world.act(ContinueAction())
world.act(ClearAction())
world.act(PickAction((5, 6)))

# Visualise the world, from the last state by default.
show_world(world)

# Take a few more actions.
world.act(AttemptSubmitAction())
world.act(SubmitAction())

# Visualise the world, from the first state, on the current screen.
show_world(world, state_no=1, screen_index=0)
```

##### Render and navigate through an individual world with real log data from a child.
```
from justhink_world import create_world, show_world, load_log

world_name = 'pretest-1'

# Load the log table for a sample (i.e. participant index) and activity.
history = load_log(sample_no=1, world_name=world_name)

# Create a world with that history.
world = create_world(world_name, history)

# Display from the first state in the log.
show_world(world, state_no=1)
```

Use `LEFT`-`RIGHT` keys to navigate to the previous and the next state respectively.


<img src="doc/example_test_log.gif" width="768" />


#### Try out a collaborative world.

How to interact with a collaborative world:
* You can suggest selecting a connection by clicking on a gold mine, dragging to another gold mine and releasing on that gold mine.
* You can agree or disagree with a suggestion by the other (robot if you are playing the human role and vice versa) by pressing the agree (check) or disagree (cross) button.


##### Visualise a collaborative world.
```
from justhink_world import create_world, show_world

# Create a world.
world = create_world('collaboration-1')

# Visualise the world on the current screen.
show_world(world, screen_index=0)
```

##### Execute actions via Python commands (and visualise the world at its latest state).
```
from justhink_world import create_world, show_world
from justhink_world.agent import Agent
from justhink_world.domain.action import SuggestPickAction, \
    AgreeAction, DisagreeAction

# Create a world.
world = create_world('collaboration-1')

# Act on the world.
world.act(SuggestPickAction((3, 1), agent=Agent.ROBOT))
world.act(AgreeAction(agent=Agent.HUMAN))
world.act(SuggestPickAction((1, 4), agent=Agent.HUMAN))
world.act(DisagreeAction(agent=Agent.ROBOT))
world.act(SuggestPickAction((4, 5), agent=Agent.ROBOT))

# Visualise the world, from the last state by default.
show_world(world)
```

##### Render and navigate through a collaborative world with real log data from a child.
```
from justhink_world import create_world, load_log, show_world

world_name = 'collaboration-1'

# Load the log table for a sample (i.e. participant index) and activity.
history = load_log(sample_no=3, world_name=world_name)

# Create a world with that history.
world = create_world(world_name, history)

show_world(world, state_no=1)
```


<img src="doc/example_collab_log.gif" width="768" />


#### Access information about a world/state.

Create a world and print the available actions with human-readible form.
```
from justhink_world import create_world

world = create_world('collaboration-1')

for action in sorted(world.agent.all_actions):
    if hasattr(action, 'edge'):
      u, v = world.env.state.network.get_edge_name(action.edge)
      action = action.__class__(edge=(u, v), agent=action.agent)
    print(action)

action_list = []
for action in sorted(world.agent.all_actions):
    if hasattr(action, 'edge'):
      u, v = world.env.state.network.get_edge_name(action.edge)
      action = action.__class__(edge=(u, v), agent=action.agent)
    print(action)
    action_list.append(action)
```

Get the node name from node id and vice versa.
```
from justhink_world import create_world
world = create_world('collaboration-1')

print(world.env.state.network.get_node_name(1))  # Prints: Luzern

print(world.env.state.network.get_node_id('Luzern'))  # Prints: 1
```

Get the edge's node's names from node id tuple and vice versa.
```
from justhink_world import create_world
world = create_world('collaboration-1')



world.env.state

print(world.env.state.network.get_edge_name((1, 2)))  # Prints: ('Luzern', 'Zurich')

print(world.env.state.network.get_edge_ids(('Luzern', 'Zurich')))  # Prints: (1, 2)
```

Print available actions at the current state (nodes are not human-readible).
```
print(world.agent.all_actions)
```

Print the current state, it's cost and MST cost.
```
state = world.cur_state
print(state, state.network.get_cost(), state.network.get_mst_cost())
```

Show both activity and mental windows:
```
from justhink_world import create_world, show_all

world = create_world('collaboration-1')
# world = create_world('pretest-1')

show_all(world)
```

Print world history.
```
print(world.history)
```

Visualise a mental state for the robot.
```
from justhink_world import create_world, show_mind, show_all

world = create_world('pretest-1')
show_mind(world)
```


## Acknowledgements

This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 765955. Namely, the [ANIMATAS Project](https://www.animatas.eu/).

## Bugs & Feature Requests

Please report bugs and request features using the [Issue Tracker](https://github.com/utku-norman/justhink_world/issues).


## References <a name="references"></a>

[1] Zheng, K., & Tellex, S. (2020). pomdp_py: A Framework to Build and Solve POMDP Problems. arXiv preprint arXiv:2004.10099.


[ROS]: http://www.ros.org
[QTrobot]: https://luxai.com
[justhink_world]: https://github.com/utku-norman/justhink_world
[justhink_scenario]: https://github.com/utku-norman/justhink_scenario
[justhink_agent]: https://github.com/utku-norman/justhink_agent
[justhink_robot]: https://github.com/utku-norman/justhink_robot
[justhink_msgs]: https://github.com/utku-norman/justhink_msgs
